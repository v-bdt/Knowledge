
>[!Objectif]
>Parser toutes les pages d'un site web et en extraire les données tels que les liens, commentaires, metadata etc. automatiquement -> Processus iteratif et automatique

- [[#Scrapers]]
- [[#Crawlers]]


# Scrapers

- [[#Scrapy]] 
- [[#Jslui]] -> Fichiers JS
- [[#Burp Engagement Tools]]

## Scrapy

Permet de scraper afin d'extraire les données.
### Install

```bash
python3 -m venv .venv;
source .venv/bin/activate;
pip3 install scrapy
```


## Jslui

https://github.com/BishopFox/jsluice

> [!Tip]
> Permet d'extraire les données des fichiers JS.

install

```sh
go install github.com/BishopFox/jsluice/cmd/jsluice@latest
jsluice -h
```

Extraire les urls des fichiers JS

```sh
jsluice urls <https://domain.fr/exemple.js> <fichier.local>
```

Mise en forme et sortie vers un fichier pour le fuzzing.

```sh
jsluice urls <https://domain.fr/exemple.js> | cut -d ":" -f 2 | cut -d "," -f 1 | sed 's/"//g' | sort -u > js_endpoints
```



## Burp Engagement Tools

> [!Tip]
> Permet d'extraire les commentaires notamment.
> Identifier si des commentaires font fuiter des informations metier / credentials

1. Clique droit sur la target -> Engagement Tools -> Find Comments




# Crawlers

- [[#Burp Crawler]]
- [[#ReconSpider]]
- [[#Katana]]


## Burp Crawler

> [!Tip]
> Disponible uniquement avec la version Pro

1. Clique droit sur le domaine dans Target -> Site map
2. Scan -> Configure scan for selected message
3. Selectionner "Crawl" et configurer la profondeur du scan




## ReconSpider

https://academy.hackthebox.com/storage/modules/144/ReconSpider.v1.2.zip

Permet de récupérer les urls du site.

Version de hackthebox qui inclue scrapy et extrait les données automatiquement.

```bash
wget -O ReconSpider.zip https://academy.hackthebox.com/storage/modules/144/ReconSpider.v1.2.zip
unzip ReconSpider.zip
```

Run

```bash
python3 ReconSpider.py http://inlanefreight.com
```




## Katana

https://github.com/projectdiscovery/katana

URL unique

```sh
katana -u https://tesla.com -headless -no-sandbox -o domain.txt
```

Liste d'URLs

```sh
katana -list URLs.txt -headless -no-sandbox -o domains.txt
```




---
# **Robots.txt / sitemap.xml**


```
<url>/robots.txt
```

```
<url>/sitemap.xml
```


---
# **.well-known**


Centralise les métadonnées critiques d'un site et est souvent présent à la racine.

Pour le registre complet des URIs -> [IANA Registry](https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml) 

### *OAuth 2.0 protocol*

Récupère la configuration de OpenID connect

```
<url>/.well-known/openid-configuration
```

### *Change password*

Récupère le lien de la page pour changer de mot de passe

```
<url>/.well-known/change-password
```

