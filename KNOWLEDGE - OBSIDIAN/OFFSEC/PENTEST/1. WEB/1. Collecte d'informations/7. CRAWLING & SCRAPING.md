
>[!Objectif]
>Parser toutes les pages d'un site web et en extraire les données tels que les liens, commentaires, metadata etc. automatiquement -> Processus iteratif et automatique


# **Scraper**

## ***Scrapy***

Permet de scraper afin d'extraire les données.
### Install

```bash
python3 -m venv .venv;
source .venv/bin/activate;
pip3 install scrapy
```


# **Crawler**
## ***ReconSpider***

Permet de récupérer les urls du site.

Version de hackthebox qui inclue scrapy et extrait les données automatiquement.

```bash
wget -O ReconSpider.zip https://academy.hackthebox.com/storage/modules/144/ReconSpider.v1.2.zip
unzip ReconSpider.zip
```

## **Run**

```bash
python3 ReconSpider.py http://inlanefreight.com
```



---
# **Robots.txt / sitemap.xml**


```
<url>/robots.txt
```

```
<url>/sitemap.xml
```


---
# **.well-known**


Centralise les métadonnées critiques d'un site et est souvent présent à la racine.

Pour le registre complet des URIs -> [IANA Registry](https://www.iana.org/assignments/well-known-uris/well-known-uris.xhtml) 

### *OAuth 2.0 protocol*

Récupère la configuration de OpenID connect

```
<url>/.well-known/openid-configuration
```

### *Change password*

Récupère le lien de la page pour changer de mot de passe

```
<url>/.well-known/change-password
```

